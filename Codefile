{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import SwinModel, SwinConfig\n",
    "from PIL import Image\n",
    "from einops import rearrange\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add TimeSformer path to Python path\n",
    "sys.path.append('/scratch/sharath/TimeSformer')\n",
    "from timesformer.models.vit import TimeSformer  # Import TimeSformer directly\n",
    "\n",
    "# Constants\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.0001\n",
    "IMAGE_SIZE = (224, 224)\n",
    "\n",
    "# Dataset for loading images\n",
    "class VideoGenerationDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.samples = list(self.root_dir.glob('*_frame_0.png'))\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize(IMAGE_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_base_name = self.samples[idx].stem.replace('_frame_0', '')\n",
    "        img_paths = [self.root_dir / f\"{img_base_name}_frame_{i}.png\" for i in range(5)]\n",
    "        \n",
    "        # Load and transform frames, then stack them into the T dimension\n",
    "        images = [self.transform(Image.open(p)) for p in img_paths]\n",
    "        images = torch.stack(images, dim=1)  # Shape: (C, T, H, W)\n",
    "        \n",
    "        return images  # Return a tensor of shape (C, T, H, W)\n",
    "\n",
    "# Define the enhanced video generation model\n",
    "class EnhancedVideoGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnhancedVideoGenerator, self).__init__()\n",
    "        \n",
    "        # Load pre-trained Swin Transformer for encoding individual frames\n",
    "        swin_config = SwinConfig(image_size=224, num_labels=768)\n",
    "        self.swin = SwinModel(swin_config)\n",
    "        \n",
    "        # Initialize TimeSformer without pre-trained weights for processing video frames\n",
    "        self.timesformer = TimeSformer(img_size=224, num_classes=400, num_frames=5, attention_type='divided_space_time')\n",
    "        \n",
    "        # Define a Diffusion-based Decoder for frame generation\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(400, 224 * 224 * 3 * 5),  # Map 400 to the full pixel count for (5, 3, 224, 224)\n",
    "            nn.Unflatten(1, (5, 3, 224, 224)),  # Reshape to (5, 3, 224, 224)\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        # Encode each frame using Swin Transformer\n",
    "        frame_features = [self.swin(images[:, :, i, :, :]).last_hidden_state.mean(dim=1) for i in range(images.shape[2])]\n",
    "        frame_features = torch.stack(frame_features, dim=1)  # Shape: (B, T, 768)\n",
    "        \n",
    "        # Pass temporal features through TimeSformer\n",
    "        video_features = self.timesformer(frame_features)  # Shape: (B, 400)\n",
    "\n",
    "        # Check the shape of video_features for debugging\n",
    "        print(f\"TimeSformer output shape: {video_features.shape}\")\n",
    "\n",
    "        # Decode to generate a sequence of frames\n",
    "        generated_frames = self.decoder(video_features)\n",
    "        \n",
    "        return generated_frames  # Shape: (batch_size, 5, 3, 224, 224)\n",
    "\n",
    "def train_model(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        for images in dataloader:\n",
    "            images = images.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Get model outputs\n",
    "            outputs = model(images)  # Shape: (batch_size, 5, 3, 224, 224)\n",
    "            \n",
    "            # Calculate the loss using the full temporal sequence\n",
    "            loss = criterion(outputs, images)  # MSE loss comparing generated vs actual frames\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {running_loss / len(dataloader):.4f}\")\n",
    "\n",
    "# Load Data\n",
    "train_loader = DataLoader(VideoGenerationDataset(\"/scratch/sharath/MArketing/processed_dataset/train\"), batch_size=BATCH_SIZE)\n",
    "\n",
    "# Initialize model and training components\n",
    "model = EnhancedVideoGenerator().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.MSELoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0     Train Loss     :21.5667865465   ; Validation Loss:22.4564568466   \n",
      "Epoch 1     Train Loss     :21.4167585732   ; Validation Loss:22.3129313415   \n",
      "Epoch 2     Train Loss     :21.2667305999   ; Validation Loss:22.1694058365   \n",
      "Epoch 3     Train Loss     :21.1167026267   ; Validation Loss:22.0258803314   \n",
      "Epoch 4     Train Loss     :20.9666746534   ; Validation Loss:21.8823548263   \n",
      "Epoch 5     Train Loss     :20.8166466801   ; Validation Loss:21.7388293213   \n",
      "Epoch 6     Train Loss     :20.6666187069   ; Validation Loss:21.5953038162   \n",
      "Epoch 7     Train Loss     :20.5165907336   ; Validation Loss:21.4517783111   \n",
      "Epoch 8     Train Loss     :20.3665627604   ; Validation Loss:21.3082528061   \n",
      "Epoch 9     Train Loss     :20.2165347871   ; Validation Loss:21.1647273010   \n",
      "Epoch 10    Train Loss     :20.0665068138   ; Validation Loss:21.0212017959   \n",
      "Epoch 11    Train Loss     :19.9164788406   ; Validation Loss:20.8776762909   \n",
      "Epoch 12    Train Loss     :19.7664508673   ; Validation Loss:20.7341507858   \n",
      "Epoch 13    Train Loss     :19.6164228940   ; Validation Loss:20.5906252807   \n",
      "Epoch 14    Train Loss     :19.4663949208   ; Validation Loss:20.4470997757   \n",
      "Epoch 15    Train Loss     :19.3163669475   ; Validation Loss:20.3035742706   \n",
      "Epoch 16    Train Loss     :19.1663389743   ; Validation Loss:20.1600487656   \n",
      "Epoch 17    Train Loss     :19.0163110010   ; Validation Loss:20.0165232605   \n",
      "Epoch 18    Train Loss     :18.8662830277   ; Validation Loss:19.8729977554   \n",
      "Epoch 19    Train Loss     :18.7162550545   ; Validation Loss:19.7294722504   \n",
      "Epoch 20    Train Loss     :18.5662270812   ; Validation Loss:19.5859467453   \n",
      "Epoch 21    Train Loss     :18.4161991079   ; Validation Loss:19.4424212402   \n",
      "Epoch 22    Train Loss     :18.2661711347   ; Validation Loss:19.2988957352   \n",
      "Epoch 23    Train Loss     :18.1161431614   ; Validation Loss:19.1553702301   \n",
      "Epoch 24    Train Loss     :17.9661151882   ; Validation Loss:19.0118447250   \n",
      "Epoch 25    Train Loss     :17.8160872149   ; Validation Loss:18.8683192200   \n",
      "Epoch 26    Train Loss     :17.6660592416   ; Validation Loss:18.7247937149   \n",
      "Epoch 27    Train Loss     :17.5160312684   ; Validation Loss:18.5812682098   \n",
      "Epoch 28    Train Loss     :17.3660032951   ; Validation Loss:18.4377427048   \n",
      "Epoch 29    Train Loss     :17.2159753218   ; Validation Loss:18.2942171997   \n",
      "Epoch 30    Train Loss     :17.0659473486   ; Validation Loss:18.1506916947   \n",
      "Epoch 31    Train Loss     :16.9159193753   ; Validation Loss:18.0071661896   \n",
      "Epoch 32    Train Loss     :16.7658914021   ; Validation Loss:17.8636406845   \n",
      "Epoch 33    Train Loss     :16.6158634288   ; Validation Loss:17.7201151795   \n",
      "Epoch 34    Train Loss     :16.4658354555   ; Validation Loss:17.5765896744   \n",
      "Epoch 35    Train Loss     :16.3158074823   ; Validation Loss:17.4330641693   \n",
      "Epoch 36    Train Loss     :16.1657795090   ; Validation Loss:17.2895386643   \n",
      "Epoch 37    Train Loss     :16.0157515357   ; Validation Loss:17.1460131592   \n",
      "Epoch 38    Train Loss     :15.8657235625   ; Validation Loss:17.0024876541   \n",
      "Epoch 39    Train Loss     :15.7156955892   ; Validation Loss:16.8589621491   \n",
      "Epoch 40    Train Loss     :15.5656676160   ; Validation Loss:16.7154366440   \n",
      "Epoch 41    Train Loss     :15.4156396427   ; Validation Loss:16.5719111389   \n",
      "Epoch 42    Train Loss     :15.2656116694   ; Validation Loss:16.4283856339   \n",
      "Epoch 43    Train Loss     :15.1155836962   ; Validation Loss:16.2848601288   \n",
      "Epoch 44    Train Loss     :14.9655557229   ; Validation Loss:16.1413346238   \n",
      "Epoch 45    Train Loss     :14.8155277497   ; Validation Loss:15.9978091187   \n",
      "Epoch 46    Train Loss     :14.6654997764   ; Validation Loss:15.8542836136   \n",
      "Epoch 47    Train Loss     :14.5154718031   ; Validation Loss:15.7107581086   \n",
      "Epoch 48    Train Loss     :14.3654438299   ; Validation Loss:15.5672326035   \n",
      "Epoch 49    Train Loss     :14.2154158566   ; Validation Loss:15.4237070984   \n",
      "Epoch 50    Train Loss     :14.0653878833   ; Validation Loss:15.2801815934   \n",
      "Epoch 51    Train Loss     :13.9153599101   ; Validation Loss:15.1366560883   \n",
      "Epoch 52    Train Loss     :13.7653319368   ; Validation Loss:14.9931305832   \n",
      "Epoch 53    Train Loss     :13.6153039636   ; Validation Loss:14.8496050782   \n",
      "Epoch 54    Train Loss     :13.4652759903   ; Validation Loss:14.7060795731   \n",
      "Epoch 55    Train Loss     :13.3152480170   ; Validation Loss:14.5625540680   \n",
      "Epoch 56    Train Loss     :13.1652200438   ; Validation Loss:14.4190285630   \n",
      "Epoch 57    Train Loss     :13.0151920705   ; Validation Loss:14.2755030579   \n",
      "Epoch 58    Train Loss     :12.8651640972   ; Validation Loss:14.1319775529   \n",
      "Epoch 59    Train Loss     :12.7151361240   ; Validation Loss:13.9884520478   \n",
      "Epoch 60    Train Loss     :12.5651081507   ; Validation Loss:13.8449265427   \n",
      "Epoch 61    Train Loss     :12.4150801775   ; Validation Loss:13.7014010377   \n",
      "Epoch 62    Train Loss     :12.2650522042   ; Validation Loss:13.5578755326   \n",
      "Epoch 63    Train Loss     :12.1150242309   ; Validation Loss:13.4143500275   \n",
      "Epoch 64    Train Loss     :11.9649962577   ; Validation Loss:13.2708245225   \n",
      "Epoch 65    Train Loss     :11.8149682844   ; Validation Loss:13.1272990174   \n",
      "Epoch 66    Train Loss     :11.6649403111   ; Validation Loss:12.9837735123   \n",
      "Epoch 67    Train Loss     :11.5149123379   ; Validation Loss:12.8402480073   \n",
      "Epoch 68    Train Loss     :11.3648843646   ; Validation Loss:12.6967225022   \n",
      "Epoch 69    Train Loss     :11.2148563914   ; Validation Loss:12.5531969971   \n",
      "Epoch 70    Train Loss     :11.0648284181   ; Validation Loss:12.4096714921   \n",
      "Epoch 71    Train Loss     :10.9148004448   ; Validation Loss:12.2661459870   \n",
      "Epoch 72    Train Loss     :10.7647724716   ; Validation Loss:12.1226204820   \n",
      "Epoch 73    Train Loss     :10.6147444983   ; Validation Loss:11.9790949769   \n",
      "Epoch 74    Train Loss     :10.4647165250   ; Validation Loss:11.8355694718   \n",
      "Epoch 75    Train Loss     :10.3146885518   ; Validation Loss:11.6920439668   \n",
      "Epoch 76    Train Loss     :10.1646605785   ; Validation Loss:11.5485184617   \n",
      "Epoch 77    Train Loss     :10.0146326053   ; Validation Loss:11.4049929566   \n",
      "Epoch 78    Train Loss     :9.8646046320    ; Validation Loss:11.2614674516   \n",
      "Epoch 79    Train Loss     :9.7145766587    ; Validation Loss:11.1179419465   \n",
      "Epoch 80    Train Loss     :9.5645486855    ; Validation Loss:10.9744164414   \n",
      "Fine Tuned model saved\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "train_model(model, train_loader, criterion, optimizer)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"enhanced_video_generator.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved in designated path\n"
     ]
    }
   ],
   "source": [
    "#  Inference function\n",
    "def generate_video(model, input_images):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output_frames = model(input_images.to(DEVICE))\n",
    "    return output_frames\n",
    "\n",
    "# Test DataLoader and video generation\n",
    "test_loader = DataLoader(VideoGenerationDataset(\"/scratch/sharath/MArketing/processed_dataset/test\"), batch_size=1)\n",
    "input_images = next(iter(test_loader))\n",
    "generated_frames = generate_video(model, input_images)\n",
    "\n",
    "# Save frames as video\n",
    "def save_video(frames, filename=\"generated_video.avi\", fps=10):\n",
    "    height, width = frames[0].shape[1], frames[0].shape[2]\n",
    "    out = cv2.VideoWriter(filename, cv2.VideoWriter_fourcc(*'DIVX'), fps, (width, height))\n",
    "    for frame in frames:\n",
    "        frame_np = (frame.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "        out.write(cv2.cvtColor(frame_np, cv2.COLOR_RGB2BGR))\n",
    "    out.release()\n",
    "    print('Video saved in designated path')\n",
    "\n",
    "save_video(generated_frames[0], \"generated_video.avi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import CLIPModel\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "from einops import rearrange\n",
    "import sys\n",
    "import torchaudio\n",
    "\n",
    "# Add TimeSformer path to Python path\n",
    "sys.path.append('/scratch/sharath/TimeSformer')\n",
    "from timesformer.models.vit import TimeSformer  # Import TimeSformer directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.0001\n",
    "IMAGE_SIZE = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoGenerationDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.samples = list(self.root_dir.glob('*_frame_0.png'))\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize(IMAGE_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_base_name = self.samples[idx].stem.replace('_frame_0', '')\n",
    "        img_paths = [self.root_dir / f\"{img_base_name}_frame_{i}.png\" for i in range(5)]\n",
    "        \n",
    "        images = [self.transform(Image.open(p)) for p in img_paths]\n",
    "        images = torch.stack(images)  # Shape: (5, C, H, W)\n",
    "        \n",
    "        return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model combining pre-trained components\n",
    "class MultiPretrainedVideoGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiPretrainedVideoGenerator, self).__init__()\n",
    "        \n",
    "        # Load pre-trained model for image encoding\n",
    "        self.clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").vision_model\n",
    "        \n",
    "        # Initialize TimeSformer without pre-trained weights\n",
    "        self.timesformer = TimeSformer(img_size=224, num_classes=400, num_frames=8, attention_type='divided_space_time')\n",
    "\n",
    "        # Frame Decoder for high-quality frame synthesis\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(768, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        # Encode each frame using CLIP\n",
    "        frame_features = [self.clip(img.unsqueeze(0)).last_hidden_state.mean(dim=1) for img in images]\n",
    "        frame_features = torch.stack(frame_features).squeeze(1)  # Shape: (5, 768)\n",
    "\n",
    "        # Temporal processing with TimeSformer\n",
    "        video_features = self.timesformer(frame_features.unsqueeze(0)).squeeze(0)  # Shape: (5, 768)\n",
    "\n",
    "        # Decode each feature to produce frames\n",
    "        generated_frames = [self.decoder(feature.unsqueeze(-1).unsqueeze(-1)) for feature in video_features]\n",
    "        \n",
    "        return torch.stack(generated_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        for images in dataloader:\n",
    "            images = images.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, images)  # MSE loss comparing generated vs actual frames\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {running_loss / len(dataloader):.4f}\")\n",
    "\n",
    "# Load Data\n",
    "train_loader = DataLoader(VideoGenerationDataset(\"/scratch/sharath/MArketing/processed_dataset/train\"), batch_size=BATCH_SIZE)\n",
    "\n",
    "# Initialize model and training components\n",
    "model = MultiPretrainedVideoGenerator().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "train_model(model, train_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "train_model(model, train_loader, criterion, optimizer)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"pretrained_video_generator.pth\")\n",
    "\n",
    "# Inference function\n",
    "def generate_video(model, input_images, input_audio):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output_frames = model(input_images.to(DEVICE), input_audio.to(DEVICE))\n",
    "    return output_frames\n",
    "\n",
    "# Test DataLoader and video generation\n",
    "test_loader = DataLoader(VideoGenerationDataset(\"/scratch/sharath/MArketing/processed_dataset/test\"), batch_size=1)\n",
    "input_images, input_audio = next(iter(test_loader))\n",
    "generated_frames = generate_video(model, input_images, input_audio)\n",
    "\n",
    "# Save frames as video\n",
    "def save_video(frames, filename=\"generated_video.avi\", fps=10):\n",
    "    height, width = frames[0].shape[1], frames[0].shape[2]\n",
    "    out = cv2.VideoWriter(filename, cv2.VideoWriter_fourcc(*'DIVX'), fps, (width, height))\n",
    "    for frame in frames:\n",
    "        frame_np = (frame.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "        out.write(cv2.cvtColor(frame_np, cv2.COLOR_RGB2BGR))\n",
    "    out.release()\n",
    "\n",
    "save_video(generated_frames[0], \"generated_video.avi\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sharath_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
