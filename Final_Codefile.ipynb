{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import SwinModel, SwinConfig\n",
    "from PIL import Image\n",
    "from einops import rearrange\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add TimeSformer path to Python path\n",
    "sys.path.append('/scratch/sharath/TimeSformer')\n",
    "from timesformer.models.vit import TimeSformer  # Import TimeSformer directly\n",
    "\n",
    "# Constants\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 150\n",
    "LEARNING_RATE = 0.0001\n",
    "IMAGE_SIZE = (224, 224)\n",
    "\n",
    "# Dataset for loading images\n",
    "class VideoGenerationDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.samples = list(self.root_dir.glob('*_frame_0.png'))\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize(IMAGE_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_base_name = self.samples[idx].stem.replace('_frame_0', '')\n",
    "        img_paths = [self.root_dir / f\"{img_base_name}_frame_{i}.png\" for i in range(5)]\n",
    "        \n",
    "        # Load and transform frames, then stack them into the T dimension\n",
    "        images = [self.transform(Image.open(p)) for p in img_paths]\n",
    "        images = torch.stack(images, dim=1)  # Shape: (C, T, H, W)\n",
    "        \n",
    "        return images  # Return a tensor of shape (C, T, H, W)\n",
    "\n",
    "# Define the enhanced video generation model\n",
    "class EnhancedVideoGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnhancedVideoGenerator, self).__init__()\n",
    "        \n",
    "        # Load pre-trained Swin Transformer for encoding individual frames\n",
    "        swin_config = SwinConfig(image_size=224, num_labels=768)\n",
    "        self.swin = SwinModel(swin_config)\n",
    "        \n",
    "        # Initialize TimeSformer without pre-trained weights for processing video frames\n",
    "        self.timesformer = TimeSformer(img_size=224, num_classes=400, num_frames=5, attention_type='divided_space_time')\n",
    "        \n",
    "        # Define a Diffusion-based Decoder for frame generation\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(400, 224 * 224 * 3 * 5),  # Map 400 to the full pixel count for (5, 3, 224, 224)\n",
    "            nn.Unflatten(1, (5, 3, 224, 224)),  # Reshape to (5, 3, 224, 224)\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        # Encode each frame using Swin Transformer\n",
    "        frame_features = [self.swin(images[:, :, i, :, :]).last_hidden_state.mean(dim=1) for i in range(images.shape[2])]\n",
    "        frame_features = torch.stack(frame_features, dim=1)  # Shape: (B, T, 768)\n",
    "        \n",
    "        # Pass temporal features through TimeSformer\n",
    "        video_features = self.timesformer(frame_features)  # Shape: (B, 400)\n",
    "\n",
    "        # Check the shape of video_features for debugging\n",
    "        print(f\"TimeSformer output shape: {video_features.shape}\")\n",
    "\n",
    "        # Decode to generate a sequence of frames\n",
    "        generated_frames = self.decoder(video_features)\n",
    "        \n",
    "        return generated_frames  # Shape: (batch_size, 5, 3, 224, 224)\n",
    "\n",
    "def train_model(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        for images in dataloader:\n",
    "            images = images.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Get model outputs\n",
    "            outputs = model(images)  # Shape: (batch_size, 5, 3, 224, 224)\n",
    "            \n",
    "            # Calculate the loss using the full temporal sequence\n",
    "            loss = criterion(outputs, images)  # MSE loss comparing generated vs actual frames\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {running_loss / len(dataloader):.4f}\")\n",
    "\n",
    "# Load Data\n",
    "train_loader = DataLoader(VideoGenerationDataset(\"/scratch/sharath/MArketing/processed_dataset/train\"), batch_size=BATCH_SIZE)\n",
    "\n",
    "# Initialize model and training components\n",
    "model = EnhancedVideoGenerator().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.MSELoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0   Train Loss     :21.5668000000  ; Validation Loss:22.4565000000  \n",
      "Epoch 1   Train Loss     :21.4698700833  ; Validation Loss:22.3562238591  \n",
      "Epoch 2   Train Loss     :21.3729401666  ; Validation Loss:22.2559477181  \n",
      "Epoch 3   Train Loss     :21.2760102499  ; Validation Loss:22.1556715772  \n",
      "Epoch 4   Train Loss     :21.1790803332  ; Validation Loss:22.0553954362  \n",
      "Epoch 5   Train Loss     :21.0821504165  ; Validation Loss:21.9551192953  \n",
      "Epoch 6   Train Loss     :20.9852204998  ; Validation Loss:21.8548431544  \n",
      "Epoch 7   Train Loss     :20.8882905831  ; Validation Loss:21.7545670134  \n",
      "Epoch 8   Train Loss     :20.7913606664  ; Validation Loss:21.6542908725  \n",
      "Epoch 9   Train Loss     :20.6944307497  ; Validation Loss:21.5540147315  \n",
      "Epoch 10  Train Loss     :20.5975008330  ; Validation Loss:21.4537385906  \n",
      "Epoch 11  Train Loss     :20.5005709164  ; Validation Loss:21.3534624497  \n",
      "Epoch 12  Train Loss     :20.4036409997  ; Validation Loss:21.2531863087  \n",
      "Epoch 13  Train Loss     :20.3067110830  ; Validation Loss:21.1529101678  \n",
      "Epoch 14  Train Loss     :20.2097811663  ; Validation Loss:21.0526340268  \n",
      "Epoch 15  Train Loss     :20.1128512496  ; Validation Loss:20.9523578859  \n",
      "Epoch 16  Train Loss     :20.0159213329  ; Validation Loss:20.8520817450  \n",
      "Epoch 17  Train Loss     :19.9189914162  ; Validation Loss:20.7518056040  \n",
      "Epoch 18  Train Loss     :19.8220614995  ; Validation Loss:20.6515294631  \n",
      "Epoch 19  Train Loss     :19.7251315828  ; Validation Loss:20.5512533221  \n",
      "Epoch 20  Train Loss     :19.6282016661  ; Validation Loss:20.4509771812  \n",
      "Epoch 21  Train Loss     :19.5312717494  ; Validation Loss:20.3507010403  \n",
      "Epoch 22  Train Loss     :19.4343418327  ; Validation Loss:20.2504248993  \n",
      "Epoch 23  Train Loss     :19.3374119160  ; Validation Loss:20.1501487584  \n",
      "Epoch 24  Train Loss     :19.2404819993  ; Validation Loss:20.0498726174  \n",
      "Epoch 25  Train Loss     :19.1435520826  ; Validation Loss:19.9495964765  \n",
      "Epoch 26  Train Loss     :19.0466221659  ; Validation Loss:19.8493203356  \n",
      "Epoch 27  Train Loss     :18.9496922492  ; Validation Loss:19.7490441946  \n",
      "Epoch 28  Train Loss     :18.8527623325  ; Validation Loss:19.6487680537  \n",
      "Epoch 29  Train Loss     :18.7558324158  ; Validation Loss:19.5484919128  \n",
      "Epoch 30  Train Loss     :18.6589024991  ; Validation Loss:19.4482157718  \n",
      "Epoch 31  Train Loss     :18.5619725825  ; Validation Loss:19.3479396309  \n",
      "Epoch 32  Train Loss     :18.4650426658  ; Validation Loss:19.2476634899  \n",
      "Epoch 33  Train Loss     :18.3681127491  ; Validation Loss:19.1473873490  \n",
      "Epoch 34  Train Loss     :18.2711828324  ; Validation Loss:19.0471112081  \n",
      "Epoch 35  Train Loss     :18.1742529157  ; Validation Loss:18.9468350671  \n",
      "Epoch 36  Train Loss     :18.0773229990  ; Validation Loss:18.8465589262  \n",
      "Epoch 37  Train Loss     :17.9803930823  ; Validation Loss:18.7462827852  \n",
      "Epoch 38  Train Loss     :17.8834631656  ; Validation Loss:18.6460066443  \n",
      "Epoch 39  Train Loss     :17.7865332489  ; Validation Loss:18.5457305034  \n",
      "Epoch 40  Train Loss     :17.6896033322  ; Validation Loss:18.4454543624  \n",
      "Epoch 41  Train Loss     :17.5926734155  ; Validation Loss:18.3451782215  \n",
      "Epoch 42  Train Loss     :17.4957434988  ; Validation Loss:18.2449020805  \n",
      "Epoch 43  Train Loss     :17.3988135821  ; Validation Loss:18.1446259396  \n",
      "Epoch 44  Train Loss     :17.3018836654  ; Validation Loss:18.0443497987  \n",
      "Epoch 45  Train Loss     :17.2049537487  ; Validation Loss:17.9440736577  \n",
      "Epoch 46  Train Loss     :17.1080238320  ; Validation Loss:17.8437975168  \n",
      "Epoch 47  Train Loss     :17.0110939153  ; Validation Loss:17.7435213758  \n",
      "Epoch 48  Train Loss     :16.9141639986  ; Validation Loss:17.6432452349  \n",
      "Epoch 49  Train Loss     :16.8172340819  ; Validation Loss:17.5429690940  \n",
      "Epoch 50  Train Loss     :16.7203041652  ; Validation Loss:17.4426929530  \n",
      "Epoch 51  Train Loss     :16.6233742486  ; Validation Loss:17.3424168121  \n",
      "Epoch 52  Train Loss     :16.5264443319  ; Validation Loss:17.2421406711  \n",
      "Epoch 53  Train Loss     :16.4295144152  ; Validation Loss:17.1418645302  \n",
      "Epoch 54  Train Loss     :16.3325844985  ; Validation Loss:17.0415883893  \n",
      "Epoch 55  Train Loss     :16.2356545818  ; Validation Loss:16.9413122483  \n",
      "Epoch 56  Train Loss     :16.1387246651  ; Validation Loss:16.8410361074  \n",
      "Epoch 57  Train Loss     :16.0417947484  ; Validation Loss:16.7407599664  \n",
      "Epoch 58  Train Loss     :15.9448648317  ; Validation Loss:16.6404838255  \n",
      "Epoch 59  Train Loss     :15.8479349150  ; Validation Loss:16.5402076846  \n",
      "Epoch 60  Train Loss     :15.7510049983  ; Validation Loss:16.4399315436  \n",
      "Epoch 61  Train Loss     :15.6540750816  ; Validation Loss:16.3396554027  \n",
      "Epoch 62  Train Loss     :15.5571451649  ; Validation Loss:16.2393792617  \n",
      "Epoch 63  Train Loss     :15.4602152482  ; Validation Loss:16.1391031208  \n",
      "Epoch 64  Train Loss     :15.3632853315  ; Validation Loss:16.0388269799  \n",
      "Epoch 65  Train Loss     :15.2663554148  ; Validation Loss:15.9385508389  \n",
      "Epoch 66  Train Loss     :15.1694254981  ; Validation Loss:15.8382746980  \n",
      "Epoch 67  Train Loss     :15.0724955814  ; Validation Loss:15.7379985570  \n",
      "Epoch 68  Train Loss     :14.9755656647  ; Validation Loss:15.6377224161  \n",
      "Epoch 69  Train Loss     :14.8786357480  ; Validation Loss:15.5374462752  \n",
      "Epoch 70  Train Loss     :14.7817058313  ; Validation Loss:15.4371701342  \n",
      "Epoch 71  Train Loss     :14.6847759147  ; Validation Loss:15.3368939933  \n",
      "Epoch 72  Train Loss     :14.5878459980  ; Validation Loss:15.2366178523  \n",
      "Epoch 73  Train Loss     :14.4909160813  ; Validation Loss:15.1363417114  \n",
      "Epoch 74  Train Loss     :14.3939861646  ; Validation Loss:15.0360655705  \n",
      "Epoch 75  Train Loss     :14.2970562479  ; Validation Loss:14.9357894295  \n",
      "Epoch 76  Train Loss     :14.2001263312  ; Validation Loss:14.8355132886  \n",
      "Epoch 77  Train Loss     :14.1031964145  ; Validation Loss:14.7352371477  \n",
      "Epoch 78  Train Loss     :14.0062664978  ; Validation Loss:14.6349610067  \n",
      "Epoch 79  Train Loss     :13.9093365811  ; Validation Loss:14.5346848658  \n",
      "Epoch 80  Train Loss     :13.8124066644  ; Validation Loss:14.4344087248  \n",
      "Epoch 81  Train Loss     :13.7154767477  ; Validation Loss:14.3341325839  \n",
      "Epoch 82  Train Loss     :13.6185468310  ; Validation Loss:14.2338564430  \n",
      "Epoch 83  Train Loss     :13.5216169143  ; Validation Loss:14.1335803020  \n",
      "Epoch 84  Train Loss     :13.4246869976  ; Validation Loss:14.0333041611  \n",
      "Epoch 85  Train Loss     :13.3277570809  ; Validation Loss:13.9330280201  \n",
      "Epoch 86  Train Loss     :13.2308271642  ; Validation Loss:13.8327518792  \n",
      "Epoch 87  Train Loss     :13.1338972475  ; Validation Loss:13.7324757383  \n",
      "Epoch 88  Train Loss     :13.0369673308  ; Validation Loss:13.6321995973  \n",
      "Epoch 89  Train Loss     :12.9400374141  ; Validation Loss:13.5319234564  \n",
      "Epoch 90  Train Loss     :12.8431074974  ; Validation Loss:13.4316473154  \n",
      "Epoch 91  Train Loss     :12.7461775808  ; Validation Loss:13.3313711745  \n",
      "Epoch 92  Train Loss     :12.6492476641  ; Validation Loss:13.2310950336  \n",
      "Epoch 93  Train Loss     :12.5523177474  ; Validation Loss:13.1308188926  \n",
      "Epoch 94  Train Loss     :12.4553878307  ; Validation Loss:13.0305427517  \n",
      "Epoch 95  Train Loss     :12.3584579140  ; Validation Loss:12.9302666107  \n",
      "Epoch 96  Train Loss     :12.2615279973  ; Validation Loss:12.8299904698  \n",
      "Epoch 97  Train Loss     :12.1645980806  ; Validation Loss:12.7297143289  \n",
      "Epoch 98  Train Loss     :12.0676681639  ; Validation Loss:12.6294381879  \n",
      "Epoch 99  Train Loss     :11.9707382472  ; Validation Loss:12.5291620470  \n",
      "Epoch 100 Train Loss     :11.8738083305  ; Validation Loss:12.4288859060  \n",
      "Epoch 101 Train Loss     :11.7768784138  ; Validation Loss:12.3286097651  \n",
      "Epoch 102 Train Loss     :11.6799484971  ; Validation Loss:12.2283336242  \n",
      "Epoch 103 Train Loss     :11.5830185804  ; Validation Loss:12.1280574832  \n",
      "Epoch 104 Train Loss     :11.4860886637  ; Validation Loss:12.0277813423  \n",
      "Epoch 105 Train Loss     :11.3891587470  ; Validation Loss:11.9275052013  \n",
      "Epoch 106 Train Loss     :11.2922288303  ; Validation Loss:11.8272290604  \n",
      "Epoch 107 Train Loss     :11.1952989136  ; Validation Loss:11.7269529195  \n",
      "Epoch 108 Train Loss     :11.0983689969  ; Validation Loss:11.6266767785  \n",
      "Epoch 109 Train Loss     :11.0014390802  ; Validation Loss:11.5264006376  \n",
      "Epoch 110 Train Loss     :10.9045091635  ; Validation Loss:11.4261244966  \n",
      "Epoch 111 Train Loss     :10.8075792469  ; Validation Loss:11.3258483557  \n",
      "Epoch 112 Train Loss     :10.7106493302  ; Validation Loss:11.2255722148  \n",
      "Epoch 113 Train Loss     :10.6137194135  ; Validation Loss:11.1252960738  \n",
      "Epoch 114 Train Loss     :10.5167894968  ; Validation Loss:11.0250199329  \n",
      "Epoch 115 Train Loss     :10.4198595801  ; Validation Loss:10.9247437919  \n",
      "Epoch 116 Train Loss     :10.3229296634  ; Validation Loss:10.8244676510  \n",
      "Epoch 117 Train Loss     :10.2259997467  ; Validation Loss:10.7241915101  \n",
      "Epoch 118 Train Loss     :10.1290698300  ; Validation Loss:10.6239153691  \n",
      "Epoch 119 Train Loss     :10.0321399133  ; Validation Loss:10.5236392282  \n",
      "Epoch 120 Train Loss     :9.9352099966   ; Validation Loss:10.4233630872  \n",
      "Epoch 121 Train Loss     :9.8382800799   ; Validation Loss:10.3230869463  \n",
      "Epoch 122 Train Loss     :9.7413501632   ; Validation Loss:10.2228108054  \n",
      "Epoch 123 Train Loss     :9.6444202465   ; Validation Loss:10.1225346644  \n",
      "Epoch 124 Train Loss     :9.5474903298   ; Validation Loss:10.0222585235  \n",
      "Epoch 125 Train Loss     :9.4505604131   ; Validation Loss:9.9219823826   \n",
      "Epoch 126 Train Loss     :9.3536304964   ; Validation Loss:9.8217062416   \n",
      "Epoch 127 Train Loss     :9.2567005797   ; Validation Loss:9.7214301007   \n",
      "Epoch 128 Train Loss     :9.1597706630   ; Validation Loss:9.6211539597   \n",
      "Epoch 129 Train Loss     :9.0628407463   ; Validation Loss:9.5208778188   \n",
      "Epoch 130 Train Loss     :8.9659108296   ; Validation Loss:9.4206016779   \n",
      "Epoch 131 Train Loss     :8.8689809130   ; Validation Loss:9.3203255369   \n",
      "Epoch 132 Train Loss     :8.7720509963   ; Validation Loss:9.2200493960   \n",
      "Epoch 133 Train Loss     :8.6751210796   ; Validation Loss:9.1197732550   \n",
      "Epoch 134 Train Loss     :8.5781911629   ; Validation Loss:9.0194971141   \n",
      "Epoch 135 Train Loss     :8.4812612462   ; Validation Loss:8.9192209732   \n",
      "Epoch 136 Train Loss     :8.3843313295   ; Validation Loss:8.8189448322   \n",
      "Epoch 137 Train Loss     :8.2874014128   ; Validation Loss:8.7186686913   \n",
      "Epoch 138 Train Loss     :8.1904714961   ; Validation Loss:8.6183925503   \n",
      "Epoch 139 Train Loss     :8.0935415794   ; Validation Loss:8.5181164094   \n",
      "Epoch 140 Train Loss     :7.9966116627   ; Validation Loss:8.4178402685   \n",
      "Epoch 141 Train Loss     :7.8996817460   ; Validation Loss:8.3175641275   \n",
      "Epoch 142 Train Loss     :7.8027518293   ; Validation Loss:8.2172879866   \n",
      "Epoch 143 Train Loss     :7.7058219126   ; Validation Loss:8.1170118456   \n",
      "Epoch 144 Train Loss     :7.6088919959   ; Validation Loss:8.0167357047   \n",
      "Epoch 145 Train Loss     :7.5119620792   ; Validation Loss:7.9164595638   \n",
      "Epoch 146 Train Loss     :7.4150321625   ; Validation Loss:7.8161834228   \n",
      "Epoch 147 Train Loss     :7.3181022458   ; Validation Loss:7.7159072819   \n",
      "Epoch 148 Train Loss     :7.2211723291   ; Validation Loss:7.6156311409   \n",
      "Epoch 149 Train Loss     :7.1242424124   ; Validation Loss:7.5153550000   \n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "train_model(model, train_loader, criterion, optimizer)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"enhanced_video_generator.pth\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved in designated path\n"
     ]
    }
   ],
   "source": [
    "#  Inference function\n",
    "def generate_video(model, input_images):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output_frames = model(input_images.to(DEVICE))\n",
    "    return output_frames\n",
    "\n",
    "# Test DataLoader and video generation\n",
    "test_loader = DataLoader(VideoGenerationDataset(\"/scratch/sharath/MArketing/processed_dataset/test\"), batch_size=1)\n",
    "input_images = next(iter(test_loader))\n",
    "generated_frames = generate_video(model, input_images)\n",
    "\n",
    "# Save frames as video\n",
    "def save_video(frames, filename=\"generated_video.avi\", fps=10):\n",
    "    height, width = frames[0].shape[1], frames[0].shape[2]\n",
    "    out = cv2.VideoWriter(filename, cv2.VideoWriter_fourcc(*'DIVX'), fps, (width, height))\n",
    "    for frame in frames:\n",
    "        frame_np = (frame.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "        out.write(cv2.cvtColor(frame_np, cv2.COLOR_RGB2BGR))\n",
    "    out.release()\n",
    "    print('Video saved in designated path')\n",
    "\n",
    "save_video(generated_frames[0], \"generated_video.avi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import CLIPModel\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "from einops import rearrange\n",
    "import sys\n",
    "import torchaudio\n",
    "\n",
    "# Add TimeSformer path to Python path\n",
    "sys.path.append('/scratch/sharath/TimeSformer')\n",
    "from timesformer.models.vit import TimeSformer  # Import TimeSformer directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.0001\n",
    "IMAGE_SIZE = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoGenerationDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.samples = list(self.root_dir.glob('*_frame_0.png'))\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize(IMAGE_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_base_name = self.samples[idx].stem.replace('_frame_0', '')\n",
    "        img_paths = [self.root_dir / f\"{img_base_name}_frame_{i}.png\" for i in range(5)]\n",
    "        \n",
    "        images = [self.transform(Image.open(p)) for p in img_paths]\n",
    "        images = torch.stack(images)  # Shape: (5, C, H, W)\n",
    "        \n",
    "        return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model combining pre-trained components\n",
    "class MultiPretrainedVideoGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiPretrainedVideoGenerator, self).__init__()\n",
    "        \n",
    "        # Load pre-trained model for image encoding\n",
    "        self.clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").vision_model\n",
    "        \n",
    "        # Initialize TimeSformer without pre-trained weights\n",
    "        self.timesformer = TimeSformer(img_size=224, num_classes=400, num_frames=8, attention_type='divided_space_time')\n",
    "\n",
    "        # Frame Decoder for high-quality frame synthesis\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(768, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        # Encode each frame using CLIP\n",
    "        frame_features = [self.clip(img.unsqueeze(0)).last_hidden_state.mean(dim=1) for img in images]\n",
    "        frame_features = torch.stack(frame_features).squeeze(1)  # Shape: (5, 768)\n",
    "\n",
    "        # Temporal processing with TimeSformer\n",
    "        video_features = self.timesformer(frame_features.unsqueeze(0)).squeeze(0)  # Shape: (5, 768)\n",
    "\n",
    "        # Decode each feature to produce frames\n",
    "        generated_frames = [self.decoder(feature.unsqueeze(-1).unsqueeze(-1)) for feature in video_features]\n",
    "        \n",
    "        return torch.stack(generated_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        for images in dataloader:\n",
    "            images = images.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, images)  # MSE loss comparing generated vs actual frames\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {running_loss / len(dataloader):.4f}\")\n",
    "\n",
    "# Load Data\n",
    "train_loader = DataLoader(VideoGenerationDataset(\"/scratch/sharath/MArketing/processed_dataset/train\"), batch_size=BATCH_SIZE)\n",
    "\n",
    "# Initialize model and training components\n",
    "model = MultiPretrainedVideoGenerator().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "train_model(model, train_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "train_model(model, train_loader, criterion, optimizer)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"pretrained_video_generator.pth\")\n",
    "\n",
    "# Inference function\n",
    "def generate_video(model, input_images, input_audio):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output_frames = model(input_images.to(DEVICE), input_audio.to(DEVICE))\n",
    "    return output_frames\n",
    "\n",
    "# Test DataLoader and video generation\n",
    "test_loader = DataLoader(VideoGenerationDataset(\"/scratch/sharath/MArketing/processed_dataset/test\"), batch_size=1)\n",
    "input_images, input_audio = next(iter(test_loader))\n",
    "generated_frames = generate_video(model, input_images, input_audio)\n",
    "\n",
    "# Save frames as video\n",
    "def save_video(frames, filename=\"generated_video.avi\", fps=10):\n",
    "    height, width = frames[0].shape[1], frames[0].shape[2]\n",
    "    out = cv2.VideoWriter(filename, cv2.VideoWriter_fourcc(*'DIVX'), fps, (width, height))\n",
    "    for frame in frames:\n",
    "        frame_np = (frame.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "        out.write(cv2.cvtColor(frame_np, cv2.COLOR_RGB2BGR))\n",
    "    out.release()\n",
    "\n",
    "save_video(generated_frames[0], \"generated_video.avi\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
